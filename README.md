# Fira: Can We Achieve Full-rank Training of LLMs Under Low-rank Constraint?

[![ArXiv](https://img.shields.io/badge/ArXiv-<2410.01623>-<COLOR>.svg)](https://arxiv.org/abs/2410.01623)



![](assests\framework.png)

## Introduction

We introduce Fira, a plug-and-play memory-efficient training framework of LLMs. 

Different from LoRA and Galore, we realize training with full-rank gradients of full-rank weights, constituting the first attempt to achieve full-rank training consistently under the low-rank constraint.



## TODOs

- [ ] Release the pra-training code
- [ ] Release the fine-tuning code
- [ ] Package our Fira into a Python library  for easy use



## Usage





## Pre-Training LLaMA on C4 dataset





## Fine-Tuning LLaMA on commensense reasoning tasks





## Citation

```
@article{chen2024firaachievefullranktraining,
      title={Fira: Can We Achieve Full-rank Training of LLMs Under Low-rank Constraint?}, 
      author={Xi Chen and Kaituo Feng and Changsheng Li and Xunhao Lai and Xiangyu Yue and Ye Yuan and Guoren Wang},
      journal={arXiv},
      year={2024},
}
```

